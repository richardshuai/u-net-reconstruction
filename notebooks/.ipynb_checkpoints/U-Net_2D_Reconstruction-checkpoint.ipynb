{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'i'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bf266ae2b409>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'i'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_2d_path = '../data/2d_data/plane_sims/plane_sim_1100_648_486_1.npy'\n",
    "planes_path = '../data/2d_data/planes/planes_1100_648_486_1.npy'\n",
    "model_weights_path = '../models/2d_models/N_1100_Y_648_X_486/test1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = '../data/2d_data/tf_records/plane_sims/plane_sims_N_10000_Y_648_X_486_1.tfrecords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shifted + scaled tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batchnorm epsilon\n",
    "BN_EPS = 1e-4\n",
    "\n",
    "class ConvBnRelu2d(layers.Layer):\n",
    "    def __init__(self, out_channels, kernel_size=(3, 3), padding='same'):\n",
    "        super(ConvBnRelu2d, self).__init__()\n",
    "        self.conv = layers.Conv2D(filters=out_channels, kernel_size=kernel_size, padding='same', use_bias=False)\n",
    "        self.bn = layers.BatchNormalization(epsilon=BN_EPS)\n",
    "        self.relu = layers.ReLU()\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class TanhScale(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(TanhScale, self).__init__()\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = K.tanh(x)\n",
    "        x = (x + 1) * 0.5\n",
    "        return x * m\n",
    "\n",
    "class StackEncoder(layers.Layer):\n",
    "    def __init__(self, y_channels, kernel_size=3):\n",
    "        super(StackEncoder, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.encode = keras.Sequential([\n",
    "            ConvBnRelu2d(y_channels, kernel_size=kernel_size, padding='same'),\n",
    "            ConvBnRelu2d(y_channels, kernel_size=kernel_size, padding='same')\n",
    "        ])\n",
    "        self.max_pool = layers.MaxPool2D(pool_size=2, strides=2)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.encode(x)\n",
    "        x_small = self.max_pool(x)\n",
    "        return x, x_small\n",
    "\n",
    "\n",
    "class StackDecoder(layers.Layer):\n",
    "    def __init__(self, y_channels, kernel_size=3):\n",
    "        super(StackDecoder, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        \n",
    "        self.decode = keras.Sequential([\n",
    "            ConvBnRelu2d(y_channels, kernel_size=kernel_size, padding='same'),\n",
    "            ConvBnRelu2d(y_channels, kernel_size=kernel_size, padding='same'),\n",
    "            ConvBnRelu2d(y_channels, kernel_size=kernel_size, padding='same'),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def call(self, x, down_tensor):\n",
    "        x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n",
    "        \n",
    "        # Calculate cropping for down_tensor to concatenate with x\n",
    "        _, h2, w2, _ = down_tensor.shape\n",
    "        _, h1, w1, _ = x.shape\n",
    "        h_diff, w_diff = h2 - h1, w2 - w1\n",
    "        \n",
    "        cropping = ((int(np.ceil(h_diff / 2)), int(np.floor(h_diff / 2))),\n",
    "                    (int(np.ceil(w_diff / 2)), int(np.floor(w_diff / 2))))\n",
    "        down_tensor = layers.Cropping2D(cropping=cropping)(down_tensor)        \n",
    "        x = layers.concatenate([x, down_tensor], axis=3)\n",
    "        x = self.decode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_2d(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(UNet_2d, self).__init__()\n",
    "\n",
    "        self.down1 = StackEncoder(24, kernel_size=3) # 256\n",
    "        self.down2 = StackEncoder(64, kernel_size=3)  # 128\n",
    "        self.down3 = StackEncoder(128, kernel_size=3)  # 64\n",
    "        self.down4 = StackEncoder(256, kernel_size=3)  # 32\n",
    "        self.down5 = StackEncoder(512, kernel_size=3)  # 16\n",
    "        \n",
    "\n",
    "        self.up5 = StackDecoder(256, kernel_size=3)  # 32\n",
    "        self.up4 = StackDecoder(128, kernel_size=3)  # 64\n",
    "        self.up3 = StackDecoder(64, kernel_size=3)  # 128\n",
    "        self.up2 = StackDecoder(24, kernel_size=3)  # 256\n",
    "        self.up1 = StackDecoder(24, kernel_size=3)  # 512\n",
    "        self.classify = layers.Conv2D(filters=1, kernel_size=1, use_bias=True)\n",
    "        \n",
    "        self.center = ConvBnRelu2d(512, kernel_size=3, padding='same')\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        out = x;\n",
    "        down1_tensor, out = self.down1(out)\n",
    "        down2_tensor, out = self.down2(out)\n",
    "        down3_tensor, out = self.down3(out)\n",
    "        down4_tensor, out = self.down4(out)\n",
    "        down5_tensor, out = self.down5(out)\n",
    "\n",
    "        out = self.center(out)\n",
    "        \n",
    "        out = self.up5(out, down5_tensor)\n",
    "        out = self.up4(out, down4_tensor)\n",
    "        out = self.up3(out, down3_tensor)\n",
    "        out = self.up2(out, down2_tensor)\n",
    "        out = self.up1(out, down1_tensor)\n",
    "\n",
    "        out = self.classify(out)\n",
    "        out = tf.squeeze(out, axis=3)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         out = self.activation_fn(out)\n",
    "#         self.activation_fn = TanhScale()\n",
    "#       self.bn = layers.BatchNormalization(epsilon=BN_EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading, Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_N_11000(x, eps):\n",
    "    return (x - MEANS) / (STDS + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_dims = (648, 486)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing for N=11000, using npz files\n",
    "sims_2d_path = '../data/2d_data/plane_sims/plane_sim_11000_648_486_1.npz'\n",
    "planes_uint_path = '../data/2d_data/planes/planes_uint_11000_648_486_1.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_arrays = np.load(sims_2d_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEANS = sims_arrays['means']\n",
    "STDS = sims_arrays['stds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bde07bede319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load in forward model simulations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msims_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msims_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'plane_sims'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msims_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msims_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msims_2d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Expand to channel dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msims_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msims_2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Cast to float16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    261\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m                     \u001b[0mread_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_read_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                     \u001b[0mread_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_count\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m                     array[i:i+read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[1;32m    779\u001b[0m                                                              count=read_count)\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# done about that.  note that regular files can't be non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eof\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIN_READ_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m             self._eof = (self._decompressor.eof or\n\u001b[1;32m    950\u001b[0m                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compress_left\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load in forward model simulations\n",
    "sims_2d = sims_arrays['plane_sims']\n",
    "n, y, x = sims_2d.shape\n",
    "sims_2d = sims_2d[..., np.newaxis] # Expand to channel dimension\n",
    "sims_2d = sims_2d.astype(np.float16) # Cast to float16\n",
    "input_shape = sims_2d.shape\n",
    "\n",
    "# Load in ground truth\n",
    "planes = np.load(planes_path)['planes']\n",
    "\n",
    "# Crop to target image size\n",
    "m = 1\n",
    "planes = planes[:, 4:644, 3:483]\n",
    "\n",
    "# Standard normalize input/ground truth\n",
    "eps = 1e-4\n",
    "\n",
    "# planes = normalize_N_11000(planes, eps)\n",
    "sims_2d = normalize_N_11000(sims_2d, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, eps):\n",
    "    return (x - np.mean(x, axis=0)) / (np.std(x, axis=0) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = tf.data.TFRecordDataset(filenames=dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'plane': tf.io.FixedLenFeature(obj_dims, tf.float32),\n",
    "    'sim': tf.io.FixedLenFeature(obj_dims, tf.float32)\n",
    "}\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = raw_dataset.map(_parse_function)\n",
    "\n",
    "dataset.shuffle(buffer_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in forward model simulations\n",
    "sims_2d = np.load(sims_2d_path)\n",
    "n, y, x = sims_2d.shape\n",
    "sims_2d = sims_2d[..., np.newaxis] # Expand to channel dimension\n",
    "sims_2d = sims_2d.astype(np.float32) # Cast to float32\n",
    "input_shape = sims_2d.shape\n",
    "\n",
    "# Load in ground truth\n",
    "planes = np.load(planes_path)\n",
    "\n",
    "# Crop to target image size\n",
    "m = 1\n",
    "planes = planes[:, 4:644, 3:483]\n",
    "\n",
    "# Standard normalize input/ground truth\n",
    "eps = 1e-4\n",
    "\n",
    "planes = normalize(planes, eps)\n",
    "sims_2d = normalize(sims_2d, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = normalize(planes, eps)\n",
    "sims_2d = normalize(sims_2d, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert planes.min() == 0.0 and planes.max() == m, \"planes min/max scaling issue\"\n",
    "# assert sims_2d.min() == 0.0 and sims_2d.max() == m, \"sims_2d min/max scaling issue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Sample input\")\n",
    "plt.imshow(sims_2d[0,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "    # Save the plot to a PNG in memory.\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    plt.close(figure)\n",
    "    buf.seek(0)\n",
    "    # Convert PNG buffer to TF image\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    # Add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    return image\n",
    "\n",
    "def scaled_mse_loss(y_actual,y_pred):\n",
    "    loss = K.square((y_actual-y_pred))\n",
    "#     loss = K.sqrt(loss)\n",
    "    loss = K.sum(loss)\n",
    "    return loss\n",
    "\n",
    "def plot_image_tensorboard(epoch, logs):\n",
    "    # Create a plot to visualize image reconstruction progress\n",
    "    i = 2\n",
    "    # Call the model to get prediction\n",
    "    pred = model.predict(sims_2d[i][np.newaxis])\n",
    "\n",
    "    # Create a mpl figure\n",
    "    figure = plt.figure(figsize=(10,10))\n",
    "\n",
    "    # Plot the prediction\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"prediction\")\n",
    "    plt.imshow(pred[0])\n",
    "    # Plot groundtruth\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"ground truth\")\n",
    "    plt.imshow(planes[i])\n",
    "    plot_image = plot_to_image(figure)\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.image(\"Prediction vs Ground Truth\", plot_image, step=epoch)\n",
    "\n",
    "import datetime\n",
    "log_dir=os.path.join('logs_2d', 'fit') + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=1000000)\n",
    "file_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "plot_image_tensorboard_cb = keras.callbacks.LambdaCallback(on_epoch_end=plot_image_tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet_2d()\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "# model.compile(optimizer=adam, loss=scaled_mse_loss, metrics=['mean_squared_error'])\n",
    "model.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "model.build(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 8\n",
    "TRAIN_LENGTH = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "\n",
    "train_x = sims_2d[:TRAIN_LENGTH]\n",
    "train_y = planes[:TRAIN_LENGTH]\n",
    "\n",
    "model.fit(x=train_x, y=train_y, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, callbacks=[plot_image_tensorboard_cb, \n",
    "                                                                                           tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_weights_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet_2d()\n",
    "model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 995\n",
    "num_predict = 10\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "pred = model.predict(sims_2d[j:j+num_predict])\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print('Prediction time per sample:', (t1 - t0) / num_predict, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 15 * num_predict))\n",
    "rows, columns = num_predict, 2\n",
    "for i in range(num_predict):\n",
    "    # Plot prediction\n",
    "    s = fig.add_subplot(rows, columns, columns*i+1)\n",
    "    s.set_title(\"prediction {0}\".format(j + i), size=20)\n",
    "    plt.imshow(pred[i])\n",
    "    \n",
    "    # Plot ground truth\n",
    "    s = fig.add_subplot(rows, columns, columns*i+2)\n",
    "    s.set_title(\"ground truth {0}\".format(j + i), size=20)\n",
    "    plt.imshow(planes[j + i])\n",
    "    \n",
    "plt.savefig('N_1000_epochs_200.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
