{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'N_11000_Y_648_X_486_1'\n",
    "input_shape = (11000, 648, 486, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import json\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_2d_path = '../data/2d_data/plane_sims/plane_sim_1100_648_486_1.npy'\n",
    "planes_path = '../data/2d_data/planes/planes_1100_648_486_1.npy'\n",
    "model_weights_path = '../models/2d_models/N_1100_Y_648_X_486/test1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = '../data/2d_data/tf_records/plane_sims/plane_sims_N_10000_Y_648_X_486_1.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "record_dir = path.join('..', 'data', '2d_data', 'tf_records', 'plane_sims')\n",
    "dataset_filename = path.join(record_dir, 'plane_sims_' + filename + '.tfrecords')\n",
    "\n",
    "# Directory to store information about dataset\n",
    "info_dir = path.join('..', 'data', '2d_data', 'tf_records', 'info')\n",
    "info_sims_file = path.join(info_dir, filename + '_sims.json')\n",
    "info_planes_file = path.join(info_dir, filename + '_planes.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shifted + scaled tanh activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batchnorm epsilon\n",
    "BN_EPS = 1e-4\n",
    "\n",
    "class ConvBnRelu2d(layers.Layer):\n",
    "    def __init__(self, out_channels, kernel_size=(3, 3), padding='same'):\n",
    "        super(ConvBnRelu2d, self).__init__()\n",
    "        self.conv = layers.Conv2D(filters=out_channels, kernel_size=kernel_size, padding='same', use_bias=False)\n",
    "        self.bn = layers.BatchNormalization(epsilon=BN_EPS)\n",
    "        self.relu = layers.ReLU()\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class TanhScale(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(TanhScale, self).__init__()\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = K.tanh(x)\n",
    "        x = (x + 1) * 0.5\n",
    "        return x * m\n",
    "\n",
    "class StackEncoder(layers.Layer):\n",
    "    def __init__(self, y_channels, kernel_size=3):\n",
    "        super(StackEncoder, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        self.encode = keras.Sequential([\n",
    "            ConvBnRelu2d(y_channels, kernel_size=kernel_size, padding='same'),\n",
    "            ConvBnRelu2d(y_channels, kernel_size=kernel_size, padding='same')\n",
    "        ])\n",
    "        self.max_pool = layers.MaxPool2D(pool_size=2, strides=2)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.encode(x)\n",
    "        x_small = self.max_pool(x)\n",
    "        return x, x_small\n",
    "\n",
    "\n",
    "class StackDecoder(layers.Layer):\n",
    "    def __init__(self, y_channels, kernel_size=3):\n",
    "        super(StackDecoder, self).__init__()\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        \n",
    "        self.decode = keras.Sequential([\n",
    "            ConvBnRelu2d(y_channels, kernel_size=kernel_size, padding='same'),\n",
    "            ConvBnRelu2d(y_channels, kernel_size=kernel_size, padding='same'),\n",
    "            ConvBnRelu2d(y_channels, kernel_size=kernel_size, padding='same'),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def call(self, x, down_tensor):\n",
    "        x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n",
    "        \n",
    "        # Calculate cropping for down_tensor to concatenate with x\n",
    "        _, h2, w2, _ = down_tensor.shape\n",
    "        _, h1, w1, _ = x.shape\n",
    "        h_diff, w_diff = h2 - h1, w2 - w1\n",
    "        \n",
    "        cropping = ((int(np.ceil(h_diff / 2)), int(np.floor(h_diff / 2))),\n",
    "                    (int(np.ceil(w_diff / 2)), int(np.floor(w_diff / 2))))\n",
    "        down_tensor = layers.Cropping2D(cropping=cropping)(down_tensor)        \n",
    "        x = layers.concatenate([x, down_tensor], axis=3)\n",
    "        x = self.decode(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_2d(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(UNet_2d, self).__init__()\n",
    "\n",
    "        self.down1 = StackEncoder(24, kernel_size=3) # 256\n",
    "        self.down2 = StackEncoder(64, kernel_size=3)  # 128\n",
    "        self.down3 = StackEncoder(128, kernel_size=3)  # 64\n",
    "        self.down4 = StackEncoder(256, kernel_size=3)  # 32\n",
    "        self.down5 = StackEncoder(512, kernel_size=3)  # 16\n",
    "        \n",
    "\n",
    "        self.up5 = StackDecoder(256, kernel_size=3)  # 32\n",
    "        self.up4 = StackDecoder(128, kernel_size=3)  # 64\n",
    "        self.up3 = StackDecoder(64, kernel_size=3)  # 128\n",
    "        self.up2 = StackDecoder(24, kernel_size=3)  # 256\n",
    "        self.up1 = StackDecoder(24, kernel_size=3)  # 512\n",
    "        self.classify = layers.Conv2D(filters=1, kernel_size=1, use_bias=True)\n",
    "        \n",
    "        self.center = ConvBnRelu2d(512, kernel_size=3, padding='same')\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        out = x;\n",
    "        down1_tensor, out = self.down1(out)\n",
    "        down2_tensor, out = self.down2(out)\n",
    "        down3_tensor, out = self.down3(out)\n",
    "        down4_tensor, out = self.down4(out)\n",
    "        down5_tensor, out = self.down5(out)\n",
    "\n",
    "        out = self.center(out)\n",
    "        \n",
    "        out = self.up5(out, down5_tensor)\n",
    "        out = self.up4(out, down4_tensor)\n",
    "        out = self.up3(out, down3_tensor)\n",
    "        out = self.up2(out, down2_tensor)\n",
    "        out = self.up1(out, down1_tensor)\n",
    "\n",
    "        out = self.classify(out)\n",
    "        out = tf.squeeze(out, axis=3)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         out = self.activation_fn(out)\n",
    "#         self.activation_fn = TanhScale()\n",
    "#       self.bn = layers.BatchNormalization(epsilon=BN_EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading, Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_N_11000(x, eps):\n",
    "    return (x - MEANS) / (STDS + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_dims = (648, 486)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing for N=11000, using npz files\n",
    "sims_2d_path = '../data/2d_data/plane_sims/plane_sim_11000_648_486_1.npz'\n",
    "planes_uint_path = '../data/2d_data/planes/planes_uint_11000_648_486_1.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_arrays = np.load(sims_2d_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEANS = sims_arrays['means']\n",
    "STDS = sims_arrays['stds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in forward model simulations\n",
    "sims_2d = sims_arrays['plane_sims']\n",
    "n, y, x = sims_2d.shape\n",
    "sims_2d = sims_2d[..., np.newaxis] # Expand to channel dimension\n",
    "sims_2d = sims_2d.astype(np.float16) # Cast to float16\n",
    "input_shape = sims_2d.shape\n",
    "\n",
    "# Load in ground truth\n",
    "planes = np.load(planes_path)['planes']\n",
    "\n",
    "# Crop to target image size\n",
    "m = 1\n",
    "planes = planes[:, 4:644, 3:483]\n",
    "\n",
    "# Standard normalize input/ground truth\n",
    "eps = 1e-4\n",
    "\n",
    "# planes = normalize_N_11000(planes, eps)\n",
    "sims_2d = normalize_N_11000(sims_2d, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecords Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(info_planes_file) as planes_f:\n",
    "    data = json.load(planes_f)\n",
    "    PLANE_MEANS = np.array(data['means'], dtype=np.float16)\n",
    "    PLANE_STDS = np.array(data['stds'], dtype=np.float16)\n",
    "\n",
    "with open(info_sims_file) as sims_f:\n",
    "    data = json.load(sims_f)\n",
    "    SIM_MEANS = np.array(data['means'], dtype=np.float16)\n",
    "    SIM_STDS = np.array(data['stds'], dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 9.090e-05 2.728e-04 ... 3.636e-04 3.636e-04 9.090e-05]\n",
      " [9.090e-05 1.818e-04 2.728e-04 ... 7.272e-04 7.272e-04 1.818e-04]\n",
      " [4.547e-04 5.455e-04 1.818e-04 ... 4.547e-04 4.547e-04 1.818e-04]\n",
      " ...\n",
      " [2.728e-04 5.455e-04 6.366e-04 ... 8.183e-04 1.091e-03 5.455e-04]\n",
      " [2.728e-04 4.547e-04 4.547e-04 ... 1.182e-03 9.093e-04 2.728e-04]\n",
      " [9.090e-05 1.818e-04 3.636e-04 ... 5.455e-04 1.818e-04 0.000e+00]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    eps = 1e-1\n",
    "    feature_description = {\n",
    "        'plane': tf.io.FixedLenFeature(obj_dims, tf.float32),\n",
    "        'sim': tf.io.FixedLenFeature(obj_dims, tf.float32)\n",
    "    }\n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    plane = example['plane']\n",
    "    plane = tf.cast(plane, tf.float16)\n",
    "    plane = (plane - PLANE_MEANS) / (PLANE_STDS + eps) # Normalizzation\n",
    "    plane = plane[4:644, 3:483] # Crop to target image size\n",
    "\n",
    "    sim = example['sim']\n",
    "    sim = tf.cast(sim, tf.float16)\n",
    "    sim = (sim - SIM_MEANS) / (SIM_STDS + eps) # Normalization\n",
    "    sim = sim[..., np.newaxis] # Expand to channel dimension\n",
    "    \n",
    "    return sim, plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filenames):\n",
    "    raw_dataset = tf.data.TFRecordDataset(filenames=dataset_filename)\n",
    "    \n",
    "    dataset = raw_dataset.map(_parse_function)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(256)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, eps):\n",
    "    return (x - np.mean(x, axis=0)) / (np.std(x, axis=0) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in forward model simulations\n",
    "sims_2d = np.load(sims_2d_path)\n",
    "n, y, x = sims_2d.shape\n",
    "sims_2d = sims_2d[..., np.newaxis] # Expand to channel dimension\n",
    "sims_2d = sims_2d.astype(np.float32) # Cast to float32\n",
    "input_shape = sims_2d.shape\n",
    "\n",
    "# Load in ground truth\n",
    "planes = np.load(planes_path)\n",
    "\n",
    "# Crop to target image size\n",
    "m = 1\n",
    "planes = planes[:, 4:644, 3:483]\n",
    "\n",
    "# Standard normalize input/ground truth\n",
    "eps = 1e-4\n",
    "\n",
    "planes = normalize(planes, eps)\n",
    "sims_2d = normalize(sims_2d, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = normalize(planes, eps)\n",
    "sims_2d = normalize(sims_2d, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert planes.min() == 0.0 and planes.max() == m, \"planes min/max scaling issue\"\n",
    "# assert sims_2d.min() == 0.0 and sims_2d.max() == m, \"sims_2d min/max scaling issue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Sample input\")\n",
    "plt.imshow(sims_2d[0,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_image(figure):\n",
    "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
    "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
    "    # Save the plot to a PNG in memory.\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    # Closing the figure prevents it from being displayed directly inside\n",
    "    # the notebook.\n",
    "    plt.close(figure)\n",
    "    buf.seek(0)\n",
    "    # Convert PNG buffer to TF image\n",
    "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "    # Add the batch dimension\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    return image\n",
    "\n",
    "def scaled_mse_loss(y_actual,y_pred):\n",
    "    loss = K.square((y_actual-y_pred))\n",
    "#     loss = K.sqrt(loss)\n",
    "    loss = K.sum(loss)\n",
    "    return loss\n",
    "\n",
    "def plot_image_tensorboard(epoch, logs):\n",
    "    # Create a plot to visualize image reconstruction progress\n",
    "    \n",
    "    # Call the model to get prediction\n",
    "    dataset_iter = iter(train_dataset)\n",
    "    example = next(dataset_iter)\n",
    "    pred = model.predict(example['sim'][np.newaxis])\n",
    "\n",
    "    # Create a mpl figure\n",
    "    figure = plt.figure(figsize=(10,10))\n",
    "\n",
    "    # Plot the prediction\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"prediction\")\n",
    "    plt.imshow(pred[0])\n",
    "    # Plot groundtruth\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"ground truth\")\n",
    "    plt.imshow(example['plane'])\n",
    "    plot_image = plot_to_image(figure)\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.image(\"Prediction vs Ground Truth\", plot_image, step=epoch)\n",
    "\n",
    "import datetime\n",
    "log_dir=os.path.join('logs_2d', 'fit') + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=1000000)\n",
    "file_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "plot_image_tensorboard_cb = keras.callbacks.LambdaCallback(on_epoch_end=plot_image_tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"u_net_2d_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "stack_encoder_10 (StackEncod multiple                  5592      \n",
      "_________________________________________________________________\n",
      "stack_encoder_11 (StackEncod multiple                  51200     \n",
      "_________________________________________________________________\n",
      "stack_encoder_12 (StackEncod multiple                  222208    \n",
      "_________________________________________________________________\n",
      "stack_encoder_13 (StackEncod multiple                  886784    \n",
      "_________________________________________________________________\n",
      "stack_encoder_14 (StackEncod multiple                  3543040   \n",
      "_________________________________________________________________\n",
      "stack_decoder_10 (StackDecod multiple                  3542016   \n",
      "_________________________________________________________________\n",
      "stack_decoder_11 (StackDecod multiple                  886272    \n",
      "_________________________________________________________________\n",
      "stack_decoder_12 (StackDecod multiple                  221952    \n",
      "_________________________________________________________________\n",
      "stack_decoder_13 (StackDecod multiple                  38304     \n",
      "_________________________________________________________________\n",
      "stack_decoder_14 (StackDecod multiple                  21024     \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           multiple                  25        \n",
      "_________________________________________________________________\n",
      "conv_bn_relu2d_77 (ConvBnRel multiple                  2361344   \n",
      "=================================================================\n",
      "Total params: 11,779,761\n",
      "Trainable params: 11,771,825\n",
      "Non-trainable params: 7,936\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = UNet_2d()\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "# model.compile(optimizer=adam, loss=scaled_mse_loss, metrics=['mean_squared_error'])\n",
    "model.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "model.build(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 8\n",
    "TRAIN_LENGTH = 10000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "\n",
    "train_x = sims_2d[:TRAIN_LENGTH]\n",
    "train_y = planes[:TRAIN_LENGTH]\n",
    "\n",
    "model.fit(x=train_x, y=train_y, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, callbacks=[plot_image_tensorboard_cb, \n",
    "                                                                                           tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecordDataset Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = 20\n",
    "VAL_LENGTH = 8\n",
    "TEST_LENGTH = 0\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 4\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "\n",
    "dataset = create_dataset(filenames=dataset_filename)\n",
    "train_dataset = dataset.take(TRAIN_LENGTH)\n",
    "\n",
    "val_dataset = dataset.skip(TRAIN_LENGTH)\n",
    "val_dataset = dataset.take(VAL_LENGTH)\n",
    "\n",
    "test_dataset = dataset.skip(VAL_LENGTH + TEST_LENGTH)\n",
    "test_dataset = dataset.take(TEST_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(4, 648, 486, 1), dtype=float16, numpy=\n",
      "array([[[[185.   ],\n",
      "         [143.8  ],\n",
      "         [152.2  ],\n",
      "         ...,\n",
      "         [163.9  ],\n",
      "         [195.9  ],\n",
      "         [282.8  ]],\n",
      "\n",
      "        [[288.5  ],\n",
      "         [284.2  ],\n",
      "         [185.5  ],\n",
      "         ...,\n",
      "         [235.   ],\n",
      "         [179.2  ],\n",
      "         [117.1  ]],\n",
      "\n",
      "        [[224.1  ],\n",
      "         [366.2  ],\n",
      "         [358.   ],\n",
      "         ...,\n",
      "         [298.8  ],\n",
      "         [281.   ],\n",
      "         [255.9  ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[817.5  ],\n",
      "         [720.5  ],\n",
      "         [408.5  ],\n",
      "         ...,\n",
      "         [669.   ],\n",
      "         [617.   ],\n",
      "         [824.5  ]],\n",
      "\n",
      "        [[276.2  ],\n",
      "         [343.2  ],\n",
      "         [341.   ],\n",
      "         ...,\n",
      "         [363.5  ],\n",
      "         [270.5  ],\n",
      "         [278.   ]],\n",
      "\n",
      "        [[173.5  ],\n",
      "         [130.6  ],\n",
      "         [257.8  ],\n",
      "         ...,\n",
      "         [307.2  ],\n",
      "         [360.5  ],\n",
      "         [194.2  ]]],\n",
      "\n",
      "\n",
      "       [[[-54.62 ],\n",
      "         [-65.9  ],\n",
      "         [ 67.56 ],\n",
      "         ...,\n",
      "         [252.2  ],\n",
      "         [ 19.4  ],\n",
      "         [-72.1  ]],\n",
      "\n",
      "        [[-45.8  ],\n",
      "         [-48.44 ],\n",
      "         [ 69.75 ],\n",
      "         ...,\n",
      "         [230.5  ],\n",
      "         [ 43.7  ],\n",
      "         [-27.7  ]],\n",
      "\n",
      "        [[ 32.78 ],\n",
      "         [ 19.89 ],\n",
      "         [ 78.4  ],\n",
      "         ...,\n",
      "         [222.9  ],\n",
      "         [ 19.7  ],\n",
      "         [-11.26 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 47.88 ],\n",
      "         [118.8  ],\n",
      "         [ 95.1  ],\n",
      "         ...,\n",
      "         [343.5  ],\n",
      "         [211.5  ],\n",
      "         [ 32.94 ]],\n",
      "\n",
      "        [[  9.53 ],\n",
      "         [ 69.2  ],\n",
      "         [ 90.5  ],\n",
      "         ...,\n",
      "         [204.6  ],\n",
      "         [ 38.72 ],\n",
      "         [-54.34 ]],\n",
      "\n",
      "        [[-44.1  ],\n",
      "         [-12.414],\n",
      "         [ 60.72 ],\n",
      "         ...,\n",
      "         [222.1  ],\n",
      "         [-20.11 ],\n",
      "         [-54.78 ]]],\n",
      "\n",
      "\n",
      "       [[[446.2  ],\n",
      "         [389.   ],\n",
      "         [317.8  ],\n",
      "         ...,\n",
      "         [254.5  ],\n",
      "         [103.06 ],\n",
      "         [180.1  ]],\n",
      "\n",
      "        [[264.8  ],\n",
      "         [429.2  ],\n",
      "         [512.5  ],\n",
      "         ...,\n",
      "         [278.8  ],\n",
      "         [132.8  ],\n",
      "         [325.8  ]],\n",
      "\n",
      "        [[407.5  ],\n",
      "         [593.5  ],\n",
      "         [554.   ],\n",
      "         ...,\n",
      "         [311.   ],\n",
      "         [260.2  ],\n",
      "         [309.   ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 98.7  ],\n",
      "         [-55.8  ],\n",
      "         [ 93.6  ],\n",
      "         ...,\n",
      "         [125.44 ],\n",
      "         [ 53.12 ],\n",
      "         [ 47.5  ]],\n",
      "\n",
      "        [[316.8  ],\n",
      "         [415.8  ],\n",
      "         [368.5  ],\n",
      "         ...,\n",
      "         [-12.96 ],\n",
      "         [ 66.   ],\n",
      "         [180.2  ]],\n",
      "\n",
      "        [[485.8  ],\n",
      "         [425.   ],\n",
      "         [234.9  ],\n",
      "         ...,\n",
      "         [223.8  ],\n",
      "         [354.5  ],\n",
      "         [452.2  ]]],\n",
      "\n",
      "\n",
      "       [[[419.2  ],\n",
      "         [229.8  ],\n",
      "         [250.6  ],\n",
      "         ...,\n",
      "         [871.   ],\n",
      "         [783.   ],\n",
      "         [603.5  ]],\n",
      "\n",
      "        [[332.5  ],\n",
      "         [270.5  ],\n",
      "         [249.8  ],\n",
      "         ...,\n",
      "         [586.5  ],\n",
      "         [601.   ],\n",
      "         [486.8  ]],\n",
      "\n",
      "        [[249.   ],\n",
      "         [234.2  ],\n",
      "         [252.9  ],\n",
      "         ...,\n",
      "         [442.   ],\n",
      "         [427.2  ],\n",
      "         [363.5  ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[217.9  ],\n",
      "         [139.8  ],\n",
      "         [166.8  ],\n",
      "         ...,\n",
      "         [568.   ],\n",
      "         [475.8  ],\n",
      "         [399.5  ]],\n",
      "\n",
      "        [[339.   ],\n",
      "         [160.6  ],\n",
      "         [166.5  ],\n",
      "         ...,\n",
      "         [717.5  ],\n",
      "         [703.   ],\n",
      "         [630.5  ]],\n",
      "\n",
      "        [[432.8  ],\n",
      "         [186.8  ],\n",
      "         [156.8  ],\n",
      "         ...,\n",
      "         [940.5  ],\n",
      "         [906.   ],\n",
      "         [791.5  ]]]], dtype=float16)>, <tf.Tensor: shape=(4, 640, 480), dtype=float16, numpy=\n",
      "array([[[-0.002342, -0.003056, -0.00573 , ..., -0.00375 , -0.003056,\n",
      "         -0.00508 ],\n",
      "        [-0.003056, -0.002342, -0.004425, ..., -0.003056, -0.004425,\n",
      "         -0.003056],\n",
      "        [-0.004425, -0.004425, -0.003056, ..., -0.00375 , -0.00699 ,\n",
      "         -0.00508 ],\n",
      "        ...,\n",
      "        [-0.00508 , -0.00699 , -0.004425, ..., -0.00508 , -0.00375 ,\n",
      "         -0.00375 ],\n",
      "        [-0.001602, -0.00573 , -0.00573 , ..., -0.00508 , -0.00375 ,\n",
      "         -0.004425],\n",
      "        [-0.003056, -0.004425, -0.00508 , ..., -0.004425, -0.00573 ,\n",
      "         -0.00573 ]],\n",
      "\n",
      "       [[-0.002342, -0.003056, -0.00573 , ..., -0.00375 , -0.003056,\n",
      "         -0.00508 ],\n",
      "        [-0.003056, -0.002342, -0.004425, ..., -0.003056, -0.004425,\n",
      "         -0.003056],\n",
      "        [-0.004425, -0.004425, -0.003056, ..., -0.00375 , -0.00699 ,\n",
      "         -0.00508 ],\n",
      "        ...,\n",
      "        [-0.00508 , -0.00699 , -0.004425, ..., -0.00508 , -0.00375 ,\n",
      "         -0.00375 ],\n",
      "        [-0.001602, -0.00573 , -0.00573 , ..., -0.00508 , -0.00375 ,\n",
      "         -0.004425],\n",
      "        [-0.003056, -0.004425, -0.00508 , ..., -0.004425, -0.00573 ,\n",
      "         -0.00573 ]],\n",
      "\n",
      "       [[-0.002342, -0.003056, -0.00573 , ..., -0.00375 , -0.003056,\n",
      "         -0.00508 ],\n",
      "        [-0.003056, -0.002342, -0.004425, ..., -0.003056, -0.004425,\n",
      "         -0.003056],\n",
      "        [-0.004425, -0.004425, -0.003056, ..., -0.00375 , -0.00699 ,\n",
      "         -0.00508 ],\n",
      "        ...,\n",
      "        [-0.00508 , -0.00699 , -0.004425, ..., -0.00508 , -0.00375 ,\n",
      "         -0.00375 ],\n",
      "        [-0.001602, -0.00573 , -0.00573 , ..., -0.00508 , -0.00375 ,\n",
      "         -0.004425],\n",
      "        [-0.003056, -0.004425, -0.00508 , ..., -0.004425, -0.00573 ,\n",
      "         -0.00573 ]],\n",
      "\n",
      "       [[-0.002342, -0.003056, -0.00573 , ..., -0.00375 , -0.003056,\n",
      "         -0.00508 ],\n",
      "        [-0.003056, -0.002342, -0.004425, ..., -0.003056, -0.004425,\n",
      "         -0.003056],\n",
      "        [-0.004425, -0.004425, -0.003056, ..., -0.00375 , -0.00699 ,\n",
      "         -0.00508 ],\n",
      "        ...,\n",
      "        [-0.00508 , -0.00699 , -0.004425, ..., -0.00508 , -0.00375 ,\n",
      "         -0.00375 ],\n",
      "        [-0.001602, -0.00573 , -0.00573 , ..., -0.00508 , -0.00375 ,\n",
      "         -0.004425],\n",
      "        [-0.003056, -0.004425, -0.00508 , ..., -0.004425, -0.00573 ,\n",
      "         -0.00573 ]]], dtype=float16)>)\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 5 steps, validate for 5 steps\n",
      "Epoch 1/200\n",
      "2/5 [===========>..................] - ETA: 21s - loss: 0.9351 - mean_squared_error: 0.9351"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m       \u001b[0;32myield\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-b0e1f573b561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, \n\u001b[0;32m----> 2\u001b[0;31m           callbacks=[plot_image_tensorboard_cb, tensorboard_callback], validation_data=val_dataset, validation_steps=5)\n\u001b[0m",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m                       total_epochs=1)\n\u001b[1;32m    396\u001b[0m                   cbks.make_logs(model, epoch_logs, eval_result, ModeKeys.TEST,\n\u001b[0;32m--> 397\u001b[0;31m                                  prefix='val_')\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mon_epoch\u001b[0;34m(self, epoch, mode)\u001b[0m\n\u001b[1;32m    769\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/u-net-reconstruction/richardenv/lib/python3.6/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-e15fb5c266bd>\u001b[0m in \u001b[0;36mplot_image_tensorboard\u001b[0;34m(epoch, logs)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mdataset_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Create a mpl figure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, \n",
    "          callbacks=[plot_image_tensorboard_cb, tensorboard_callback], validation_data=val_dataset, validation_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_weights_path, save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet_2d()\n",
    "model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 995\n",
    "num_predict = 10\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "pred = model.predict(sims_2d[j:j+num_predict])\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "print('Prediction time per sample:', (t1 - t0) / num_predict, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30, 15 * num_predict))\n",
    "rows, columns = num_predict, 2\n",
    "for i in range(num_predict):\n",
    "    # Plot prediction\n",
    "    s = fig.add_subplot(rows, columns, columns*i+1)\n",
    "    s.set_title(\"prediction {0}\".format(j + i), size=20)\n",
    "    plt.imshow(pred[i])\n",
    "    \n",
    "    # Plot ground truth\n",
    "    s = fig.add_subplot(rows, columns, columns*i+2)\n",
    "    s.set_title(\"ground truth {0}\".format(j + i), size=20)\n",
    "    plt.imshow(planes[j + i])\n",
    "    \n",
    "plt.savefig('N_1000_epochs_200.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
